import os
import json
import time
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
from django.core.management.base import BaseCommand
from django.contrib.auth import get_user_model
from django.conf import settings
from posts.models import Post
from recommendations.models import UserEmbedding, UserRecommendation

User = get_user_model()

NODE2VEC_DIM = 128
OPENAI_DIM = 3072
MAX_POSTS = 50

class AttentionPooling(nn.Module):
def **init**(self, post_dim=OPENAI_DIM, acc_dim=NODE2VEC_DIM, n_head=8):
super().**init**()
self.mha = nn.MultiheadAttention(post_dim, n_head, batch_first=True, dropout=0.1)
self.ln  = nn.LayerNorm(post_dim)
self.proj = nn.Sequential(
nn.Linear(post_dim, acc_dim*2),
nn.GELU(),
nn.Linear(acc_dim*2, acc_dim)
)
self.sc  = nn.Linear(post_dim, 1, bias=False)

```
def forward(self, x, mask=None):
    x = F.normalize(x, p=2, dim=-1)
    att, _ = self.mha(x, x, x, key_padding_mask=(mask==0) if mask is not None else None)
    x = self.ln(x + att)
    scr = self.sc(x).squeeze(-1)
    if mask is not None:
        scr = scr.masked_fill(mask==0, -1e9)
    w = F.softmax(scr, dim=-1)
    acc = torch.sum(x * w.unsqueeze(-1), dim=1)
    return F.normalize(self.proj(acc), p=2, dim=-1), w
```

class FollowPredictor(nn.Module):
def **init**(self, acc_dim=NODE2VEC_DIM, hid=256):
super().**init**()
self.fe = nn.Sequential(nn.Linear(acc_dim, hid), nn.GELU())
self.te = nn.Sequential(nn.Linear(acc_dim, hid), nn.GELU())
self.head = nn.Sequential(
nn.Linear(hid*3, hid), nn.GELU(),
nn.Linear(hid, hid//2), nn.GELU(),
nn.Linear(hid//2, 1), nn.Sigmoid()
)

```
def forward(self, f, t):
    f, t = self.fe(f), self.te(t)
    return self.head(torch.cat([f, t, f*t], -1)).squeeze(-1)
```

class EndToEndFollowModel(nn.Module):
def **init**(self, post_dim=OPENAI_DIM, account_dim=NODE2VEC_DIM, hidden_dim=256):
super().**init**()
self.attention_pooling = AttentionPooling(post_dim, account_dim)
self.follow_predictor = FollowPredictor(account_dim, hidden_dim)

```
def forward(self, follower_posts, followee_posts, follower_masks=None, followee_masks=None):
    follower_vector, follower_attn = self.attention_pooling(follower_posts, follower_masks)
    followee_vector, followee_attn = self.attention_pooling(followee_posts, followee_masks)
    follow_prob = self.follow_predictor(follower_vector, followee_vector)
    return follow_prob, follower_vector, followee_vector, (follower_attn, followee_attn)
```

class Command(BaseCommand):
help = â€˜ãƒ­ã‚°ç©ã¿ç‰ˆï¼šattention_pooling_follow_model.ptã‚’ä½¿ã£ã¦æŠ•ç¨¿ãƒ™ã‚¯ãƒˆãƒ«â†’ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆâ†’ãƒ•ã‚©ãƒ­ãƒ¼æ¨è–¦ã¾ã§å…¨ã¦ãƒ¯ãƒ³ã‚¹ãƒˆãƒƒãƒ—ã§å‡¦ç†ã—ã¾ã™ã€‚â€™

```
def add_arguments(self, parser):
    parser.add_argument('--force', action='store_true', help='æ—¢å­˜ã®UserEmbeddingã‚’å…¨å‰Šé™¤ã—ã¦å†ç”Ÿæˆ')
    parser.add_argument('--top_k', type=int, default=10, help='ãƒªã‚³ãƒ¡ãƒ³ãƒ‰ä¸Šä½ä»¶æ•°')

def log(self, message, level="INFO"):
    """ãƒ­ã‚°å‡ºåŠ›ãƒ˜ãƒ«ãƒ‘ãƒ¼"""
    timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
    formatted_message = f"[{timestamp}] [{level}] {message}"
    print(formatted_message)
    self.stdout.write(formatted_message)

def get_memory_usage(self):
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
    try:
        import psutil
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        return f"{memory_mb:.1f}MB"
    except ImportError:
        return "psutilæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
    except Exception as e:
        return f"å–å¾—ã‚¨ãƒ©ãƒ¼: {e}"

def check_torch_health(self, device):
    """PyTorchã®å‹•ä½œç¢ºèª"""
    try:
        self.log("PyTorchå‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        test_tensor = torch.randn(10, 10).to(device)
        result = torch.mm(test_tensor, test_tensor.T)
        self.log(f"PyTorchå‹•ä½œç¢ºèª: OK (çµæœå½¢çŠ¶: {result.shape})")
        return True
    except Exception as e:
        self.log(f"PyTorchå‹•ä½œç¢ºèª: å¤±æ•— - {e}", "ERROR")
        return False

def validate_model_structure(self, model):
    """ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®ç¢ºèª"""
    try:
        self.log("ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®æ¤œè¨¼ä¸­...")
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        self.log(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
        self.log(f"å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
        
        if hasattr(model, 'attention_pooling'):
            self.log("âœ“ attention_pooling ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç¢ºèª")
        if hasattr(model, 'follow_predictor'):
            self.log("âœ“ follow_predictor ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç¢ºèª")
        
        return True
    except Exception as e:
        self.log(f"ãƒ¢ãƒ‡ãƒ«æ§‹é€ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
        return False

def handle(self, *args, **options):
    start_time = time.time()
    force = options.get('force', False)
    top_k = options.get('top_k', 10)
    
    self.log("=" * 80)
    self.log("ğŸš€ ãƒ­ã‚°ç©ã¿ç‰ˆæ¨è–¦ç”Ÿæˆã‚³ãƒãƒ³ãƒ‰é–‹å§‹")
    self.log(f"ğŸ“Š ã‚ªãƒ—ã‚·ãƒ§ãƒ³: force={force}, top_k={top_k}")
    self.log(f"ğŸ’¾ åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {self.get_memory_usage()}")
    self.log("=" * 80)

    try:
        # 1. ç’°å¢ƒæƒ…å ±ã®ç¢ºèª
        self.log("ğŸ” STEP 1: ç’°å¢ƒæƒ…å ±ã®ç¢ºèªé–‹å§‹")
        self.log(f"ğŸ“ BASE_DIR: {settings.BASE_DIR}")
        self.log(f"ğŸ“ å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}")
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')
        self.log(f"ğŸ–¥ï¸  ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
        
        if torch.cuda.is_available():
            self.log(f"ğŸ”¥ CUDA ãƒ‡ãƒã‚¤ã‚¹æ•°: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                self.log(f"ğŸ”¥ CUDA ãƒ‡ãƒã‚¤ã‚¹ {i}: {torch.cuda.get_device_name(i)}")
                memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                self.log(f"ğŸ”¥ CUDA ãƒ¡ãƒ¢ãƒªç·é‡: {memory_total:.1f}GB")
        
        if not self.check_torch_health(device):
            self.log("âŒ PyTorchå‹•ä½œç¢ºèªã«å¤±æ•—ã—ã¾ã—ãŸ", "ERROR")
            return
        
        # 2. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
        self.log("ğŸ” STEP 2: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèªé–‹å§‹")
        model_path = os.path.join(settings.BASE_DIR, 'recommendations', 'pretrained', 'attention_pooling_follow_model.pt')
        self.log(f"ğŸ“„ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {model_path}")
        
        if not os.path.exists(model_path):
            self.log(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {model_path}", "ERROR")
            parent_dir = os.path.dirname(model_path)
            if os.path.exists(parent_dir):
                files = os.listdir(parent_dir)
                self.log(f"ğŸ“‚ {parent_dir} å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«: {files}")
            return
        
        file_size = os.path.getsize(model_path)
        self.log(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)")
        
        with open(model_path, 'rb') as f:
            header = f.read(100)
            self.log(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼ (æœ€åˆ50ãƒã‚¤ãƒˆ): {header[:50]}")
            
            try:
                header_str = header.decode('utf-8')
                if 'version https://git-lfs.github.com' in header_str:
                    self.log("âš ï¸  Git LFSãƒã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼", "WARNING")
                    self.log("ğŸ’¡ 'git lfs pull' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„", "WARNING")
                    return
            except UnicodeDecodeError:
                self.log("âœ… ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªï¼ˆæ­£å¸¸ãªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰")

        # 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±ã®ç¢ºèª
        self.log("ğŸ” STEP 3: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±ã®ç¢ºèªé–‹å§‹")
        user_count = User.objects.filter(is_staff=False, is_superuser=False).count()
        total_user_count = User.objects.count()
        post_count = Post.objects.count()
        embedding_count = UserEmbedding.objects.count()
        recommendation_count = UserRecommendation.objects.count()
        
        self.log(f"ğŸ‘¥ ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {user_count}")
        self.log(f"ğŸ‘¥ å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {total_user_count}")
        self.log(f"ğŸ“ æŠ•ç¨¿æ•°: {post_count}")
        self.log(f"ğŸ§  æ—¢å­˜UserEmbeddingæ•°: {embedding_count}")
        self.log(f"â­ æ—¢å­˜æ¨è–¦æ•°: {recommendation_count}")

        posts_with_embedding = Post.objects.filter(embedding__isnull=False).count()
        posts_without_embedding = post_count - posts_with_embedding
        self.log(f"ğŸ§  ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ä»˜ãæŠ•ç¨¿æ•°: {posts_with_embedding}")
        self.log(f"âŒ ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç„¡ã—æŠ•ç¨¿æ•°: {posts_without_embedding}")
        
        if posts_with_embedding == 0:
            self.log("âš ï¸  æŠ•ç¨¿ã«ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãŒã‚ã‚Šã¾ã›ã‚“ï¼å…ˆã«OpenAIåŸ‹ã‚è¾¼ã¿ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„", "WARNING")

        # 4. ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        self.log("ğŸ” STEP 4: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿é–‹å§‹")
        model_load_start = time.time()
        
        try:
            self.log("ğŸ—ï¸  ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆä¸­...")
            model = EndToEndFollowModel()
            self.log("âœ… ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆå®Œäº†")
            
            if not self.validate_model_structure(model):
                return
            
            self.log("ğŸ“¥ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿ä¸­...")
            ckpt = torch.load(model_path, map_location=device)
            self.log(f"âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {type(ckpt)}")
            
            if isinstance(ckpt, dict):
                self.log(f"ğŸ”‘ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚­ãƒ¼æ•°: {len(ckpt)}")
                self.log(f"ğŸ”‘ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚­ãƒ¼ä¾‹: {list(ckpt.keys())[:5]}")
                
                for i, (k, v) in enumerate(list(ckpt.items())[:3]):
                    if hasattr(v, 'shape'):
                        self.log(f"ğŸ”‘ {k}: {v.shape}")
                    if i >= 2:
                        break
            
            self.log("ğŸ”„ ã‚­ãƒ¼åå¤‰æ›å‡¦ç†é–‹å§‹...")
            new_ckpt = {}
            conversion_count = 0
            
            for k, v in ckpt.items():
                if k.startswith('ap.'):
                    new_key = k.replace('ap.', 'attention_pooling.')
                    new_ckpt[new_key] = v
                    self.log(f"ğŸ”„ ã‚­ãƒ¼å¤‰æ›: {k} -> {new_key}")
                    conversion_count += 1
                elif k.startswith('fp.'):
                    new_key = k.replace('fp.', 'follow_predictor.')
                    new_ckpt[new_key] = v
                    self.log(f"ğŸ”„ ã‚­ãƒ¼å¤‰æ›: {k} -> {new_key}")
                    conversion_count += 1
                else:
                    new_ckpt[k] = v
            
            self.log(f"âœ… ã‚­ãƒ¼å¤‰æ›å®Œäº†: {conversion_count}å€‹ã®ã‚­ãƒ¼ã‚’å¤‰æ›")
            self.log(f"ğŸ“Š å¤‰æ›å¾Œã®ã‚­ãƒ¼æ•°: {len(new_ckpt)}")
            
            self.log("ğŸ“¥ ãƒ¢ãƒ‡ãƒ«ã«state_dictèª­ã¿è¾¼ã¿ä¸­...")
            model.load_state_dict(new_ckpt)
            self.log("âœ… state_dictèª­ã¿è¾¼ã¿å®Œäº†")
            
            self.log(f"ğŸ–¥ï¸  ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒã‚¤ã‚¹ {device} ã«è»¢é€ä¸­...")
            model.to(device)
            model.eval()
            self.log("âœ… ãƒ¢ãƒ‡ãƒ«è¨­å®šå®Œäº†ï¼ˆè©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆï¼‰")
            
            model_load_time = time.time() - model_load_start
            self.log(f"ğŸ‰ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å…¨ä½“å®Œäº†ï¼ å‡¦ç†æ™‚é–“: {model_load_time:.2f}ç§’")
            self.log(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¾Œãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {self.get_memory_usage()}")
            
        except Exception as e:
            self.log(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            import traceback
            error_details = traceback.format_exc()
            self.log(f"ğŸ“‹ è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±:", "ERROR")
            for line in error_details.split('\n'):
                if line.strip():
                    self.log(f"   {line}", "ERROR")
            return

        # 5. UserEmbeddingã®å†ç”Ÿæˆ
        self.log("ğŸ” STEP 5: UserEmbeddingå‡¦ç†é–‹å§‹")
        if force:
            self.log("ğŸ§¹ UserEmbeddingå¼·åˆ¶å†ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
            deleted_count = UserEmbedding.objects.count()
            UserEmbedding.objects.all().delete()
            self.log(f"ğŸ—‘ï¸  æ—¢å­˜UserEmbedding {deleted_count}ä»¶ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        else:
            self.log("ğŸ”„ UserEmbeddingå¢—åˆ†æ›´æ–°ãƒ¢ãƒ¼ãƒ‰ï¼ˆæ—¢å­˜ã¯ä¿æŒï¼‰")
        
        embedding_start_time = time.time()
        self.generate_user_embeddings(device, model)
        embedding_time = time.time() - embedding_start_time
        self.log(f"â±ï¸  UserEmbeddingå‡¦ç†æ™‚é–“: {embedding_time:.2f}ç§’")

        # 6. ãƒ•ã‚©ãƒ­ãƒ¼æ¨è–¦è¨ˆç®—
        self.log("ğŸ” STEP 6: ãƒ•ã‚©ãƒ­ãƒ¼æ¨è–¦è¨ˆç®—é–‹å§‹")
        recommendation_start_time = time.time()
        self.generate_recommendations(device, model, top_k)
        recommendation_time = time.time() - recommendation_start_time
        self.log(f"â±ï¸  æ¨è–¦è¨ˆç®—å‡¦ç†æ™‚é–“: {recommendation_time:.2f}ç§’")

        final_embedding_count = UserEmbedding.objects.count()
        final_recommendation_count = UserRecommendation.objects.count()
        
        total_time = time.time() - start_time
        self.log("=" * 80)
        self.log("ğŸ‰ å…¨å‡¦ç†å®Œäº†ï¼")
        self.log(f"â±ï¸  ç·å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
        self.log(f"ğŸ§  æœ€çµ‚UserEmbeddingæ•°: {final_embedding_count}")
        self.log(f"â­ æœ€çµ‚æ¨è–¦æ•°: {final_recommendation_count}")
        self.log(f"ğŸ’¾ æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {self.get_memory_usage()}")
        self.log("=" * 80)

    except KeyboardInterrupt:
        self.log("â¸ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­", "WARNING")
    except Exception as e:
        self.log(f"ğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
        import traceback
        error_details = traceback.format_exc()
        self.log("ğŸ“‹ è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±:", "ERROR")
        for line in error_details.split('\n'):
            if line.strip():
                self.log(f"   {line}", "ERROR")

def pad_posts(self, posts, max_length=MAX_POSTS):
    """æŠ•ç¨¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°"""
    if len(posts) == 0:
        return np.zeros((max_length, OPENAI_DIM), dtype=np.float32), np.zeros(max_length)
    
    if len(posts) >= max_length:
        return np.array(posts[:max_length]), np.ones(max_length)
    else:
        padded = np.zeros((max_length, OPENAI_DIM), dtype=np.float32)
        mask = np.zeros(max_length)
        padded[:len(posts)] = np.array(posts)
        mask[:len(posts)] = 1
        return padded, mask

def generate_user_embeddings(self, device, model):
    """æŠ•ç¨¿ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
    self.log("ğŸ§  ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆå‡¦ç†é–‹å§‹")
    
    users = User.objects.filter(is_staff=False, is_superuser=False)
    total_users = users.count()
    self.log(f"ğŸ‘¥ å‡¦ç†å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {total_users}")
    
    if total_users == 0:
        self.log("âš ï¸  å‡¦ç†å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå­˜åœ¨ã—ã¾ã›ã‚“", "WARNING")
        return
    
    processed_count = 0
    skipped_count = 0
    error_count = 0
    
    for i, user in enumerate(users):
        try:
            if i % 10 == 0 and i > 0:
                progress = (i / total_users) * 100
                self.log(f"ğŸ“Š é€²æ—: {i}/{total_users} ({progress:.1f}%) - å‡¦ç†æ¸ˆã¿: {processed_count}, ã‚¹ã‚­ãƒƒãƒ—: {skipped_count}, ã‚¨ãƒ©ãƒ¼: {error_count}")
            
            posts = Post.objects.filter(user=user)
            post_count = posts.count()
            
            if post_count == 0:
                if i < 5:
                    self.log(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user.username} (ID:{user.id}): æŠ•ç¨¿ãªã—")
                skipped_count += 1
                continue
            
            post_vectors = []
            valid_posts = 0
            invalid_posts = 0
            
            for post in posts:
                if (hasattr(post, 'embedding') and post.embedding and 
                    post.embedding.vector and isinstance(post.embedding.vector, list) and 
                    len(post.embedding.vector) == OPENAI_DIM):
                    
                    vector = np.array(post.embedding.vector, dtype=np.float32)
                    vector_norm = np.linalg.norm(vector)
                    if vector_norm > 0:
                        normalized_vector = vector / vector_norm
                        post_vectors.append(normalized_vector)
                        valid_posts += 1
                    else:
                        invalid_posts += 1
                else:
                    invalid_posts += 1
            
            if not post_vectors:
                if i < 5:
                    self.log(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user.username} (ID:{user.id}): æŠ•ç¨¿{post_count}ä»¶ã‚ã‚‹ãŒã€æœ‰åŠ¹ãªãƒ™ã‚¯ãƒˆãƒ«0ä»¶")
                skipped_count += 1
                continue
            
            if i < 5:
                self.log(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user.username} (ID:{user.id}): æŠ•ç¨¿{post_count}ä»¶ä¸­ã€æœ‰åŠ¹{valid_posts}ä»¶ã€ç„¡åŠ¹{invalid_posts}ä»¶")
            
            padded_posts, mask = self.pad_posts(post_vectors)
            
            with torch.no_grad():
                posts_tensor = torch.tensor(padded_posts, dtype=torch.float32).unsqueeze(0).to(device)
                mask_tensor = torch.tensor(mask, dtype=torch.float32).unsqueeze(0).to(device)
                
                account_vector, attention_weights = model.attention_pooling(posts_tensor, mask_tensor)
                account_vector_cpu = account_vector.squeeze(0).cpu().numpy()
                
                vector_norm = np.linalg.norm(account_vector_cpu)
                vector_mean = np.mean(account_vector_cpu)
                
                if i < 3:
                    self.log(f"ğŸ“Š ãƒ™ã‚¯ãƒˆãƒ«çµ±è¨ˆ - norm: {vector_norm:.4f}, mean: {vector_mean:.4f}")
                
                account_vector_list = account_vector_cpu.tolist()
            
            user_embedding, created = UserEmbedding.objects.update_or_create(
                user=user,
                defaults={'node2vec_vector': account_vector_list}
            )
            
            action = "æ–°è¦ä½œæˆ" if created else "æ›´æ–°"
            if i < 5:
                self.log(f"âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user.username}: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«{action}å®Œäº†")
            
            processed_count += 1
            
            if i % 50 == 0 and i > 0:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
        except Exception as e:
            error_count += 1
            self.log(f"âŒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user.username} (ID:{user.id}) å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            if error_count <= 3:
                import traceback
                self.log(f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}", "ERROR")
            continue
    
    self.log(f"ğŸ¯ ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆå®Œäº†")
    self.log(f"ğŸ“Š çµæœ: å‡¦ç†æˆåŠŸ={processed_count}, ã‚¹ã‚­ãƒƒãƒ—={skipped_count}, ã‚¨ãƒ©ãƒ¼={error_count}")
    self.log(f"ğŸ’¾ å‡¦ç†å¾Œãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {self.get_memory_usage()}")

def generate_recommendations(self, device, model, top_k):
    """ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«é–“ã®ãƒ•ã‚©ãƒ­ãƒ¼æ¨è–¦è¨ˆç®—"""
    self.log("â­ ãƒ•ã‚©ãƒ­ãƒ¼æ¨è–¦è¨ˆç®—å‡¦ç†é–‹å§‹")
    
    embeddings = UserEmbedding.objects.exclude(node2vec_vector__isnull=True)
    embedding_count = embeddings.count()
    self.log(f"ğŸ§  ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«ä¿æœ‰ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {embedding_count}")
    
    if embedding_count == 0:
        self.log("âš ï¸  ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“", "WARNING")
        return
    
    user_vectors = {}
    vector_load_errors = 0
    
    for embedding in embeddings:
        try:
            vector = np.array(embedding.node2vec_vector, dtype=np.float32)
            if vector.shape[0] != NODE2VEC_DIM:
                self.log(f"âš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ {embedding.user.id}: ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒä¸æ­£ {vector.shape}", "WARNING")
                continue
            user_vectors[str(embedding.user.id)] = vector
        except Exception as e:
            vector_load_errors += 1
            if vector_load_errors <= 3:
                self.log(f"âŒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ {embedding.user.id} ã®ãƒ™ã‚¯ãƒˆãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
    
    users = list(user_vectors.keys())
    valid_user_count = len(users)
    self.log(f"âœ… æ¨è–¦è¨ˆç®—å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {valid_user_count}")
    self.log(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼æ•°: {vector_load_errors}")
    
    if valid_user_count == 0:
        self.log("âŒ æœ‰åŠ¹ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“", "ERROR")
        return

    total_recommendations = 0
    error_users = 0
    
    for i, user_id in enumerate(users):
        try:
            if i % 10 == 0 and i > 0:
                progress = (i / valid_user_count) * 100
                self.log(f"ğŸ“Š æ¨è–¦è¨ˆç®—é€²æ—: {i}/{valid_user_count} ({progress:.1f}%) - ç”Ÿæˆæ¸ˆã¿æ¨è–¦: {total_recommendations}")
            
            candidates = [uid for uid in users if uid != user_id]
            if not candidates:
                self.log(f"âš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id}: æ¨è–¦å€™è£œãŒå­˜åœ¨ã—ã¾ã›ã‚“", "WARNING")
                continue
                
            user_vec = user_vectors[user_id]
            results = []
            
            batch_size = 100
            
            for j in range(0, len(candidates), batch_size):
                batch_candidates = candidates[j:j + batch_size]
                
                user_batch = torch.tensor([user_vec] * len(batch_candidates), dtype=torch.float32, device=device)
                candidate_batch = torch.tensor([user_vectors[c_id] for c_id in batch_candidates], dtype=torch.float32, device=device)
                
                with torch.no_grad():
                    probs = model.follow_predictor(user_batch, candidate_batch)
                    probs_cpu = probs.cpu().numpy()
                    
                    for k, c_id in enumerate(batch_candidates):
                        results.append((c_id, float(probs_cpu[k])))
            
            results.sort(key=lambda x: x[1], reverse=True)
            top_results = results[:top_k]
            
            deleted_count = UserRecommendation.objects.filter(user_id=user_id).count()
            UserRecommendation.objects.filter(user_id=user_id).delete()
            
            created_recommendations = []
            for c_id, prob in top_results:
                recommendation = UserRecommendation.objects.create(
                    user_id=user_id,
                    recommended_user_id=c_id,
                    score=prob,
                    follow_probability=round(min(100.0, prob * 100), 1),
                    uncertainty=0.0
                )
                created_recommendations.append(recommendation)
            
            if i < 5:
                self.log(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id}: {deleted_count}ä»¶å‰Šé™¤, {len(created_recommendations)}ä»¶æ–°è¦ä½œæˆ")
                if created_recommendations:
                    top_score = created_recommendations[0].score
                    self.log(f"   æœ€é«˜ã‚¹ã‚³ã‚¢: {top_score:.4f}")
            
            total_recommendations += len(created_recommendations)
            
        except Exception as e:
            error_users += 1
            self.log(f"âŒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id} ã®æ¨è–¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}", "ERROR")
            if error_users <= 3:
                import traceback
                self.log(f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}", "ERROR")
            continue
    
    self.log(f"ğŸ¯ ãƒ•ã‚©ãƒ­ãƒ¼æ¨è–¦è¨ˆç®—å®Œäº†")
    self.log(f"ğŸ“Š çµæœ: ç·æ¨è–¦æ•°={total_recommendations}, ã‚¨ãƒ©ãƒ¼ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°={error_users}")
    self.log(f"ğŸ’¾ å‡¦ç†å¾Œãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {self.get_memory_usage()}")
```